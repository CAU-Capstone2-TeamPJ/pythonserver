import re
import pytesseract
from PIL import Image
import requests
from io import BytesIO
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from extract_content import extract_body_text, extract_image_urls
from urlcrawling import get_blog_urls_with_selenium

# âœ… Tesseract ê²½ë¡œ ì§€ì •
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# âœ… NER ëª¨ë¸ ë¡œë”©
model_name = "Leo97/KoELECTRA-small-v3-modu-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# âœ… OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_images(image_urls):
    texts = []
    for url in image_urls:
        try:
            response = requests.get(url, timeout=5)
            img = Image.open(BytesIO(response.content))
            text = pytesseract.image_to_string(img, lang="kor")
            if text.strip():
                texts.append(text.strip())
        except Exception as e:
            continue
    return "\n".join(texts)

# âœ… ë³¸ë¬¸ + OCR í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì£¼ì†Œ ì •ì œ + í†µí•©)
def preprocess_text(main_text, ocr_text):
    text = main_text + "\n" + ocr_text
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# âœ… ì¥ì†Œ/ì£¼ì†Œ/ì¥ë©´ ì„¤ëª… ì¶”ì¶œ
def extract_location_info(text):
    entities = ner(text)
    location_names = [e['word'] for e in entities if e['entity_group'] == 'LOC']
    address_pattern = r'(ì„œìš¸|ë¶€ì‚°|ëŒ€ì „|ê´‘ì£¼|ëŒ€êµ¬|ì¸ì²œ|ìˆ˜ì›|ì œì£¼|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨)[^, \n]{2,}'
    addresses = re.findall(address_pattern, text)
    return list(set(location_names)), list(set(addresses))

# âœ… ì „ì²´ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
def extract_all_info_from_movie(movie_title, max_results=30):
    results = []
    urls = get_blog_urls_with_selenium(movie_title, max_results=max_results)

    # âœ… WebDriver ì¤€ë¹„
    options = options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    service = Service("C:/Users/keiro/moviecrawling/chromedriver-win64/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    for url in urls:
        try:
            print(f"[ğŸ”—] {url}")
            # âœ… ì—¬ê¸° ìˆ˜ì •: driver ê°™ì´ ë„˜ê²¨ì¤˜ì•¼ í•¨!
            body = extract_body_text(driver, url)
            images = extract_image_urls(driver)
            ocr_text = extract_text_from_images(images)
            full_text = preprocess_text(body, ocr_text)
            locs, addrs = extract_location_info(full_text)

            results.append({
                "url": url,
                "ë³¸ë¬¸ ìš”ì•½": body[:80],
                "ì´ë¯¸ì§€ OCR": ocr_text[:80],
                "ì¥ì†Œëª…": locs,
                "ì£¼ì†Œ": addrs
            })
        except Exception as e:
            print(f"[ERROR] {url}: {e}")
            continue

    driver.quit()
    return results

# âœ… ë‹¨ë… ì‹¤í–‰
if __name__ == "__main__":
    movie_title = input("ğŸ¬ ì˜í™” ì œëª© ì…ë ¥: ")
    infos = extract_all_info_from_movie(movie_title)

    print("\nğŸ“Œ ìµœì¢… ì¶”ì¶œ ê²°ê³¼:")
    for i, info in enumerate(infos, 1):
        print(f"\n[{i}] ğŸ”— {info['url']}")
        print(f"   ğŸ“– ë³¸ë¬¸ ìš”ì•½: {info['ë³¸ë¬¸ ìš”ì•½']}")
        print(f"   ğŸ–¼ï¸ OCR í…ìŠ¤íŠ¸: {info['ì´ë¯¸ì§€ OCR']}")
        print(f"   ğŸ—ºï¸ ì¥ì†Œëª…: {info['ì¥ì†Œëª…']}")
        print(f"   ğŸ“ ì£¼ì†Œ: {info['ì£¼ì†Œ']}")
