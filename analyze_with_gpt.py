import os
from openai import OpenAI
from dotenv import load_dotenv
import json
import re
from typing import List, Dict
import requests

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
ngrok_base = os.getenv("NGROK_URL")
if not ngrok_base:
    raise RuntimeError("NGROK_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
ngrok_url = ngrok_base.rstrip("/") + "/crawl"


def get_blogs_from_local_crawler(movie_title: str, max_results: int = 50) -> list[dict]:
    """
    ë¡œì»¬ í¬ë¡¤ë§ ì„œë²„(ngrok í†µí•´ ì—´ë¦¼)ì— ìš”ì²­í•˜ì—¬ ì˜í™” ë¸”ë¡œê·¸ ë³¸ë¬¸ë“¤ì„ ë°›ì•„ì˜´
    """

    payload = {
        "title": movie_title,
        "max_results": max_results
    }

    try:
        response = requests.post(ngrok_url, json=payload, timeout=30)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"[ERROR] í¬ë¡¤ë§ ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

# ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
with open("ì´ˆê¸°_í”„ë¡¬í”„íŠ¸.txt", "r", encoding="utf-8") as f:
    initial_prompt = f.read()

# ëˆ„ì  ì¥ì†Œì •ë³´ ì´ˆê¸°í™”
accumulated_result = ""

# ğŸ“Œ OpenAI ê¸°ë°˜ í•„í„°ë§ + JSON ë³€í™˜
def clean_json_text(raw: str) -> str:
    """
    ì½”ë“œ ë¸”ëŸ­(```json ... ```) ì œê±°
    """
    return re.sub(r"^```json\s*|```$", "", raw.strip(), flags=re.MULTILINE)

def filter_result_table_to_json(table_text: str) -> list:
    """
    OpenAIì—ê²Œ ë§ˆí¬ë‹¤ìš´ í‘œë¥¼ ì „ë‹¬í•˜ì—¬:
    - ì£¼ì†Œê°€ ë¶ˆë¶„ëª…í•˜ê³  ì–¸ê¸‰ ë¸”ë¡œê·¸ ìˆ˜ê°€ 1íšŒì¸ í•­ëª© ì œê±°
    - ìœ„ë„/ê²½ë„ ì¹¼ëŸ¼ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì¶”ê°€
    - ê²°ê³¼ë¥¼ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
    """

    prompt = f"""
ë‹¤ìŒì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ì˜ ì¥ì†Œ ì •ë³´ì…ë‹ˆë‹¤. ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ ì‘ì—…í•´ì£¼ì„¸ìš”:

1. ì£¼ì†Œê°€ ë¶ˆë¶„ëª…í•œ í•­ëª©ì€ ì œê±°í•©ë‹ˆë‹¤.
   - 'ì£¼ì†Œê°€ ë¶ˆë¶„ëª…í•˜ë‹¤'ì˜ ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
     ì£¼ì†Œ ë¬¸ìì—´ì— 'ë¡œ' ë˜ëŠ” 'ê¸¸'ì´ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šê±°ë‚˜ ìˆ«ìê°€ ì—†ëŠ” ê²½ìš°ì…ë‹ˆë‹¤.
     ì˜ˆ: 'ì„œìš¸ ì„±ë¶êµ¬' â†’ ë¶ˆë¶„ëª… / 'ì„œìš¸ ë§ˆí¬êµ¬ ì†ê¸°ì •ë¡œ 32' â†’ ëª…í™•í•¨
2. ë‚¨ì€ í•­ëª©ë“¤ì€ ëª¨ë‘ ìœ„ë„(latitude), ê²½ë„(longitude) í•„ë“œë¥¼ ì¶”ê°€í•˜ë˜, í˜„ì¬ëŠ” ë¹ˆ ë¬¸ìì—´("")ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
3. ê²°ê³¼ëŠ” JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
   - ë°˜ë“œì‹œ JSON í¬ë§·ë§Œ ì¶œë ¥í•˜ê³ , ì½”ë“œ ë¸”ëŸ­(```json) ì—†ì´ ì¶œë ¥í•´ì£¼ì„¸ìš”.

í‘œ:
{table_text}
"""

    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì„ êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¡œ ì •í™•íˆ ì •ì œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt.strip()}
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.2
    )

    raw_response = response.choices[0].message.content.strip()
    clean_text = clean_json_text(raw_response)

    try:
        return json.loads(clean_text)
    except json.JSONDecodeError:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨. ë‹¤ìŒ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”:\n", clean_text)
        return []

def compute_mention_rate(json_list: List[Dict], total_urls: int) -> List[Dict]:
    """
    mentionRate í•„ë“œë¥¼ ì¶”ê°€
    """
    for item in json_list:
        count = int(item.get("ì–¸ê¸‰ ë¸”ë¡œê·¸ ìˆ˜", 1))
        item["mentionRate"] = round(count / total_urls, 4) if total_urls > 0 else 0.0
    return json_list

def process_single_blog(blog_text: str, accumulated_text: str, movie_title: str):
    messages = [
        {"role": "system", "content": f"ë„ˆëŠ” ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ {movie_title} ì˜í™” ì´¬ì˜ ì¥ì†Œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ìœ ì§€í•˜ëŠ” ì „ë¬¸ê°€ì•¼." + initial_prompt},
        {"role": "user", "content": f"""ì§€ê¸ˆê¹Œì§€ ì •ë¦¬ëœ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{accumulated_text}\n\në‹¤ìŒì€ ìƒˆë¡œìš´ ë¸”ë¡œê·¸ ë³¸ë¬¸ì…ë‹ˆë‹¤:\n\n{blog_text}\n\nì´ ë³¸ë¬¸ì„ ë°˜ì˜í•´ì„œ ê²°ê³¼ë¥¼ **ì—…ë°ì´íŠ¸**í•˜ê±°ë‚˜ **ì¶”ê°€**í•´ ì£¼ì„¸ìš”."""}
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.2
    )

    return response.choices[0].message.content.strip()

def run_pipeline(all_blogs, movie_title, save_to_file=False):
    global accumulated_result
    accumulated_result = ""  # ì¤‘ìš”: API ìš”ì²­ë§ˆë‹¤ ì´ˆê¸°í™”

    for i, blog_entry in enumerate(all_blogs, 1):
        updated_result = process_single_blog(blog_entry["ë³¸ë¬¸"], accumulated_result, movie_title)
        accumulated_result = updated_result

    filtered_json = filter_result_table_to_json(accumulated_result)
    final_json = compute_mention_rate(filtered_json, total_urls=len(all_blogs))

    if save_to_file:
        output_path = f"{movie_title}_result.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(filtered_json, f, ensure_ascii=False, indent=2)

    return filtered_json

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    movie_title = input("ğŸ¬ ì˜í™” ì œëª© ì…ë ¥: ")
    all_blogs = get_blogs_from_local_crawler(movie_title, max_results=50)
    final_output = run_pipeline(all_blogs, movie_title)
