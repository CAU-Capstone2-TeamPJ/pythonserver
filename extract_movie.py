import re
import time
import pytesseract
import requests
from PIL import Image
from io import BytesIO
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import urllib.parse

# âœ… Tesseract ê²½ë¡œ ì„¤ì •
pytesseract.pytesseract.tesseract_cmd = r"C:/Program Files/Tesseract-OCR/tesseract.exe"

# âœ… NER ëª¨ë¸ ë¡œë”©
model_name = "Leo97/KoELECTRA-small-v3-modu-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")


# âœ… ìŠ¤í¬ë¡¤ ë‚´ë¦¬ëŠ” í•¨ìˆ˜
def scroll_to_bottom(driver, pause_time=1.0):
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        # ìŠ¤í¬ë¡¤ ì•„ë˜ë¡œ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause_time)

        # ìŠ¤í¬ë¡¤ í›„ ë†’ì´ í™•ì¸
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break  # ë” ì´ìƒ ì•ˆ ë‚´ë ¤ê°
        last_height = new_height
        
# âœ… ë¸”ë¡œê·¸ URL í¬ë¡¤ë§ í•¨ìˆ˜
def get_blog_urls_with_selenium(movie_title, max_results=50):
    query = f"{movie_title} ì´¬ì˜ì§€"
    encoded_query = urllib.parse.quote(query)

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    # âœ… ë¡œì»¬ìš© í¬ë¡¬ ë“œë¼ì´ë²„ ê²½ë¡œ ì„¤ì •
    chrome_driver_path = "C:/Users/keiro/moviecrawling/chromedriver-win64/chromedriver.exe"
    service = Service(executable_path=chrome_driver_path)

    driver = webdriver.Chrome(service=service, options=options)
    
    blog_links = []
    start = 1
    while len(blog_links) < max_results:
        url = f"https://search.naver.com/search.naver?where=view&query={encoded_query}&start={start}"
        print(f"[INFO] ê²€ìƒ‰ í˜ì´ì§€: {url}")
        driver.get(url)
        time.sleep(3)

        scroll_to_bottom(driver, pause_time=1.5)
        
        elements = driver.find_elements(By.CSS_SELECTOR, "a.link_tit")
        if not elements:
            print("[WARN] ê²°ê³¼ ì—†ìŒ, ì¢…ë£Œ")
            break

        before = len(blog_links)

        for e in elements:
            href = e.get_attribute("href")
            if href and href not in blog_links:
                blog_links.append(href)
            if len(blog_links) >= max_results:
                break

        after = len(blog_links)
        if after == before:
            print("[STOP] ìƒˆë¡œìš´ ë§í¬ ì—†ìŒ â†’ ì¢…ë£Œ")
            break

        start += 10

    driver.quit()
    return blog_links

# âœ… ì •ì œ í•¨ìˆ˜ (ë³¸ë¬¸ìš©)
def clean_text(text):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\u200b", "", text)
    text = re.sub(r"\b\d{2,4}[-.\s]?\d{3,4}[-.\s]?\d{4}\b", "", text)
    text = re.sub(
        r"[\U0001F600-\U0001F64F"  # emoticons
        r"\U0001F300-\U0001F5FF"
        r"\U0001F680-\U0001F6FF"
        r"\U0001F1E0-\U0001F1FF"
        r"\u2600-\u26FF"
        r"\u2700-\u27BF]+", "", text, flags=re.UNICODE)
    patterns_to_remove = [
        r"ì´\s?ê¸€ì€\s?.{0,20}ì œê³µë°›ì•˜ìŠµë‹ˆë‹¤",
        r"ë‚´\s?ëˆ\s?ë‚´\s?ì‚°",
        r"ë¸”ë¡œê·¸ì—ì„œ\s?ë”\s?ë³´ê¸°",
        r"ì¸ìŠ¤íƒ€ê·¸ë¨\s?@[\w]+",
        r"ì¢‹ì•„ìš”\s?[~!ê¾¹â™¥â¤â£ï¸]*",
        r"ë§í¬[:ï¼š]?\s?https?://[^\s]+",
        r"í´ë¦­í•´ì„œ\s?í™•ì¸í•˜ì„¸ìš”",
        r"ë”\s?ë§ì€\s?ì‚¬ì§„ì€\s?ë¸”ë¡œê·¸ì—ì„œ",
        r"ê´‘ê³ \s?í¬í•¨",
    ]
    for pattern in patterns_to_remove:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    return text.strip()

# âœ… ì´ë¯¸ì§€ URL ì¶”ì¶œ í•¨ìˆ˜

def extract_image_urls(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    img_tags = soup.find_all("img")
    valid_images = []
    for img in img_tags:
        src = img.get("src")
        if not src:
            continue
        if any(domain in src for domain in ["adimg", "doubleclick", "googlesyndication", "adsystem"]):
            continue
        if "base64" in src or src.endswith(".gif"):
            continue
        valid_images.append(src)
    return valid_images

# âœ… ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤

def extract_naver_blog_body(driver, url):
    driver.get(url)
    time.sleep(2)
    try:
        driver.switch_to.frame("mainFrame")
        time.sleep(1)
    except:
        return "[ERROR] iframe ì ‘ê·¼ ì‹¤íŒ¨", []
    soup = BeautifulSoup(driver.page_source, "html.parser")
    content = soup.select_one("div.se-main-container") or soup.select_one("#postViewArea")
    text = content.get_text(strip=True) if content else "[ë³¸ë¬¸ ì—†ìŒ]"
    return clean_text(text), extract_image_urls(driver)

def extract_general_body(driver, url):
    driver.get(url)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    selectors = [
        "div.article-body", "div.articleView", "div#article-view-content-div",
        "div.entry-content", "div#content", "article", "body"
    ]
    for sel in selectors:
        content = soup.select_one(sel)
        if content and content.get_text(strip=True):
            return clean_text(content.get_text(strip=True)), extract_image_urls(driver)
    return "[ë³¸ë¬¸ ì—†ìŒ]", []

def extract_body_text(driver, url):
    if "blog.naver.com" in url:
        return extract_naver_blog_body(driver, url)
    else:
        return extract_general_body(driver, url)

# âœ… OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_images(image_urls):
    texts = []
    for url in image_urls:
        try:
            response = requests.get(url, timeout=5)
            img = Image.open(BytesIO(response.content))
            if img.width < 100 or img.height < 100:
                continue
            text = pytesseract.image_to_string(img, lang="kor")
            if text.strip():
                texts.append(text.strip())
        except:
            continue
    return "\n".join(texts)

# âœ… í…ìŠ¤íŠ¸ í†µí•© ë° ì „ì²˜ë¦¬
def preprocess_text(main_text, ocr_text):
    text = main_text + "\n" + ocr_text
    return re.sub(r"\s+", " ", text).strip()



# âœ… ë©”ì¸ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
def extract_all_info_from_movie(movie_title, max_results=50):
    results = []
    urls = get_blog_urls_with_selenium(movie_title, max_results=max_results)

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    service = Service("C:/Users/keiro/moviecrawling/chromedriver-win64/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    for url in urls:
        try:
            print(f"[ğŸ”—] {url}")
            body, images = extract_body_text(driver, url)
            ocr_text = extract_text_from_images(images)
            full_text = preprocess_text(body, ocr_text)

            results.append({
                "url": url,
                "ë³¸ë¬¸": full_text
            })
        except Exception as e:
            print(f"[ERROR] {url}: {e}")
            continue

    driver.quit()
    return results

# âœ… ë‹¨ë… ì‹¤í–‰
if __name__ == "__main__":
    movie_title = input("ğŸ¬ ì˜í™” ì œëª© ì…ë ¥: ")
    infos = extract_all_info_from_movie(movie_title)

    print("\nğŸ“Œ ìµœì¢… ì¶”ì¶œ ê²°ê³¼:")
    for i, info in enumerate(infos, 1):
        print(f"\n[{i}] ğŸ”— {info['url']}")
        print(f"   ğŸ“– ë³¸ë¬¸: {info['ë³¸ë¬¸']}")
