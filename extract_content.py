from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import re

from urlcrawling import get_blog_urls_with_selenium  # ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€

# ğŸ“Œ ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
def clean_text(text):
    # ê¸°ë³¸ ê³µë°±, ì œë¡œí­ ë¬¸ì ì •ë¦¬
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\u200b", "", text)

    # ì „í™”ë²ˆí˜¸ ì œê±° (010-1234-5678, 02-123-4567 ë“±)
    text = re.sub(r"\b\d{2,4}[-.\s]?\d{3,4}[-.\s]?\d{4}\b", "", text)

    # ì´ëª¨í‹°ì½˜ ì œê±° (ê¸°ë³¸ì ì¸ ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ ë²”ìœ„)
    text = re.sub(
        r"[\U0001F600-\U0001F64F"  # emoticons
        r"\U0001F300-\U0001F5FF"  # symbols & pictographs
        r"\U0001F680-\U0001F6FF"  # transport & map symbols
        r"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        r"\u2600-\u26FF"          # misc symbols
        r"\u2700-\u27BF"          # dingbats
        r"]+", "", text, flags=re.UNICODE)

    # ê¸°íƒ€ ì“¸ë°ì—†ëŠ” ë¬¸ì¥ë“¤ ì œê±°
    patterns_to_remove = [
        r"ì´\s?ê¸€ì€\s?.{0,20}ì œê³µë°›ì•˜ìŠµë‹ˆë‹¤",
        r"ë‚´\s?ëˆ\s?ë‚´\s?ì‚°",
        r"ë¸”ë¡œê·¸ì—ì„œ\s?ë”\s?ë³´ê¸°",
        r"ì¸ìŠ¤íƒ€ê·¸ë¨\s?@[\w]+",
        r"ì¢‹ì•„ìš”\s?[~!ê¾¹â™¥â¤â£ï¸]*",
        r"ë§í¬[:ï¼š]?\s?https?://[^\s]+",
        r"í´ë¦­í•´ì„œ\s?í™•ì¸í•˜ì„¸ìš”",
        r"ë”\s?ë§ì€\s?ì‚¬ì§„ì€\s?ë¸”ë¡œê·¸ì—ì„œ",
        r"ê´‘ê³ \s?í¬í•¨",
    ]

    for pattern in patterns_to_remove:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)

    return text.strip()
# ğŸ“Œ ì´ë¯¸ì§€ URL í•„í„°ë§ í•¨ìˆ˜ (ê´‘ê³  ì œê±°ìš©)
def extract_image_urls(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    img_tags = soup.find_all("img")

    valid_images = []
    for img in img_tags:
        src = img.get("src")
        if not src:
            continue
        # ê´‘ê³  ì´ë¯¸ì§€ ì œì™¸ ê·œì¹™
        if any(domain in src for domain in ["adimg", "doubleclick", "googlesyndication", "adsystem"]):
            continue
        if not src.startswith("http"):
            continue
        valid_images.append(src)
    return valid_images

# ğŸ“Œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ
def extract_naver_blog_body(driver, url):
    driver.get(url)
    time.sleep(2)
    try:
        driver.switch_to.frame("mainFrame")
        time.sleep(1)
    except:
        return "[ERROR] iframe ì ‘ê·¼ ì‹¤íŒ¨", []

    soup = BeautifulSoup(driver.page_source, "html.parser")
    content = soup.select_one("div.se-main-container") or soup.select_one("#postViewArea")
    text = content.get_text(strip=True) if content else "[ë³¸ë¬¸ ì—†ìŒ]"
    cleaned = clean_text(text)
    images = extract_image_urls(driver)
    return cleaned, images

# ğŸ“Œ ì¼ë°˜ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ
def extract_general_body(driver, url):
    driver.get(url)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    selectors = [
        "div.article-body",
        "div.articleView",
        "div#article-view-content-div",
        "div.entry-content",
        "div#content",
        "article",
        "body"
    ]
    for sel in selectors:
        content = soup.select_one(sel)
        if content and content.get_text(strip=True):
            cleaned = clean_text(content.get_text(strip=True))
            images = extract_image_urls(driver)
            return cleaned, images
    return "[ë³¸ë¬¸ ì—†ìŒ]", []

# ğŸ“Œ ë³¸ë¬¸ ì¶”ì¶œ ì§„ì…ì 
def extract_body_text(driver, url):
    if "blog.naver.com" in url:
        return extract_naver_blog_body(driver, url)
    else:
        return extract_general_body(driver, url)

# ğŸ“Œ ì™¸ë¶€ì—ì„œ í˜¸ì¶œìš© (ë³¸ë¬¸ + ì´ë¯¸ì§€ URL)
def extract_text_and_images_from_url(url):
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    service = Service("C:/Users/keiro/moviecrawling/chromedriver-win64/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    try:
        text, images = extract_body_text(driver, url)
    finally:
        driver.quit()

    return text, images

# âœ… ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    movie_title = "ê¸°ìƒì¶©"
    urls = get_blog_urls_with_selenium(movie_title, max_results=10)

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    service = Service("C:/Users/keiro/moviecrawling/chromedriver-win64/chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    for i, url in enumerate(urls, 1):
        print(f"\n[{i:02d}] ğŸ”— {url}")
        try:
            text, images = extract_body_text(driver, url)
            print(f"ë³¸ë¬¸: {text[:200]}...")
            print(f"ì´ë¯¸ì§€ {len(images)}ê°œ:")
            for img_url in images:
                print(f"  - {img_url}")
        except Exception as e:
            print(f"[ERROR] {e}")

    driver.quit()
