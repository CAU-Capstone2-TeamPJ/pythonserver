from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from analyze_with_gpt import run_pipeline, get_blogs_from_local_crawler
import logging

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

app = FastAPI()

# âœ… ìš”ì²­ DTO
class MovieInfoRequestDto(BaseModel):
    id: int
    title: str
    director: str
    releaseDate: str

# âœ… ì‘ë‹µìš© ì¥ì†Œ ì •ë³´ êµ¬ì¡°
class LocationInfo(BaseModel):
    name: str
    country: str
    description: str
    nearbyKeywords: List[str]
    recommendKeywords: List[str]
    address: str
    mentionRate: float
    mentionCount: int
    durationTime: float

# âœ… ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ í†µì¼
def to_list(value):
    if isinstance(value, list):
        return value
    elif isinstance(value, str):
        return [x.strip() for x in value.split(',')]
    return []

# âœ… dict â†’ LocationInfo ë³€í™˜
def convert_to_location_info(raw_data: List[dict]) -> List[LocationInfo]:
    result = []
    for item in raw_data:
        result.append(LocationInfo(
            name=item.get("ì¥ì†Œëª…", ""),
            country=item.get("êµ­ê°€", ""),
            description=item.get("ì„¤ëª…", ""),
            nearbyKeywords=to_list(item.get("ì¶”ê°€ì •ë³´", [])),
            recommendKeywords=to_list(item.get("í‚¤ì›Œë“œ", [])),
            address=item.get("ì£¼ì†Œ", ""),
            mentionRate=float(item.get("mentionRate", 0.0)),
            mentionCount=int(item.get("ì–¸ê¸‰ ë¸”ë¡œê·¸ ìˆ˜", 1)),
            durationTime=float(item.get("ì²´ë¥˜ì‹œê°„", 0.0))
        ))
    return result

# âœ… ì‘ë‹µ DTO
class FilmingLocationResponseDto(BaseModel):
    movieId: int
    locations: List[LocationInfo]

# âœ… ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸
@app.post("/movies", response_model=FilmingLocationResponseDto)
def get_filming_locations(request: MovieInfoRequestDto):
    try:
        logger.info(f"ğŸ¬ ì˜í™” ì œëª© ìˆ˜ì‹ : {request.title}")

        # ë¸”ë¡œê·¸ ìˆ˜ì§‘
        all_blogs = get_blogs_from_local_crawler(request.title, max_results=30)
        logger.info(f"âœ… ë°›ì€ ë¸”ë¡œê·¸ ìˆ˜: {len(all_blogs)}")

        # GPT ê¸°ë°˜ ì¥ì†Œ ë¶„ì„
        raw_locations = run_pipeline(all_blogs, request.title, save_to_file=False)
        logger.info(f"ğŸ“Œ GPT íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ì¥ì†Œ í›„ë³´ ìˆ˜: {len(raw_locations)}")

        # ì •ì œ í›„ ì‘ë‹µ ë³€í™˜
        locations = convert_to_location_info(raw_locations)
        logger.info(f"ğŸ“¦ ì‘ë‹µìœ¼ë¡œ ë³´ë‚¼ ì¥ì†Œ ìˆ˜: {len(locations)}")

        return FilmingLocationResponseDto(movieId=request.id, locations=locations)

    except Exception as e:
        logger.exception("ğŸ”¥ ì˜í™” ì¥ì†Œ ì¶”ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ!")
        return {"error": "ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

# âœ… í—¬ìŠ¤ ì²´í¬ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/healthz")
def health_check():
    return {"status": "ok"}
